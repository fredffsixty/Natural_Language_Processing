{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ilFTxfgl7Sj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXBCeCV6Fryo",
        "outputId": "babc2f35-769c-476b-bf91-fadce36807fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juAPPaWFmS-y"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "# repository https://github.com/iresiragusa/NLP/tree/main\n",
        "# https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category?select=bbc-text.csv\n",
        "# scarichiamo il dataset e lo carichiamo su COLAB\n",
        "\n",
        "root = \"/content/gdrive/MyDrive/Colab Notebooks/torch/\"\n",
        "df = pd.read_csv(root+\"data/BBC-text/bbc-text.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBaXhs_hmXfD"
      },
      "outputs": [],
      "source": [
        "# il dataset è formato da 2225 sample contenenti aricoli della BBC\n",
        "# suddivisi in 5 categorie in base al loro topic\n",
        "\n",
        "print('n sample -> '+str(len(df)))\n",
        "labels = set(df['category'])\n",
        "print('categories -> '+str(labels)+'['+str(len(labels))+']')\n",
        "print(df['category'].value_counts())\n",
        "\n",
        "# associo ad ogni categoria un indice, così ho delle label numeriche\n",
        "labels_dict = {\n",
        "    'business': 0,\n",
        "    'politics': 1,\n",
        "    'tech': 2,\n",
        "    'sport': 3,\n",
        "    'entertainment': 4\n",
        "}\n",
        "\n",
        "df['labels'] = df.apply(lambda row: labels_dict[row.category], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPmyopURmgI2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x_train, x_test, y_train, y_test) = train_test_split(df['text'], df['labels'], test_size=0.2, random_state=17)\n",
        "(x_train, x_val, y_train, y_val) = train_test_split(x_train, y_train, test_size=0.1, random_state=17)\n",
        "\n",
        "# sarebbe uno split 72, 8, 20 per avere lo stesso test dell'altra volta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6nwHqTEmuKx"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, x, y, stopwords):\n",
        "\n",
        "        # x e y sono series di pandas\n",
        "        tokens_litt = [nltk.word_tokenize(text, language='english')\n",
        "         for text in list(x)]\n",
        "        text_clean = []\n",
        "\n",
        "        if stopwords:\n",
        "            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n",
        "                text_clean.append(' '.join([w.lower() for w in sentence if\n",
        "                    not w.lower() in nltk.corpus.stopwords.words(\"english\")]))\n",
        "        else:\n",
        "            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n",
        "                text_clean.append(' '.join([w.lower() for w in sentence]))\n",
        "            # ogni token è separato dall'altro con uno spazio\n",
        "\n",
        "        self.texts = text_clean\n",
        "        self.labels = [torch.tensor(label) for label in y]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.texts[idx]\n",
        "        batch_labels = np.array(self.labels[idx])\n",
        "\n",
        "        return batch_texts, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwK_BUUzlFJ-"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"epochs\": 5,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"stopwords\": False,\n",
        "    \"language_model\": \"bert-base-uncased\",\n",
        "    \"layers\": 1,\n",
        "    \"h_dim\": 768,\n",
        "    \"bilstm\": True,\n",
        "    \"patience\": 5,\n",
        "    \"min_delta\": 0.01\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnC3SzpfpWrZ"
      },
      "outputs": [],
      "source": [
        "#creo i dataset\n",
        "\n",
        "train_dataset = Dataset(x_train, y_train, hyperparameters[\"stopwords\"])\n",
        "val_dataset = Dataset(x_val, y_val, hyperparameters[\"stopwords\"])\n",
        "test_dataset = Dataset(x_test, y_test, hyperparameters[\"stopwords\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KdTmVtrnCp6"
      },
      "outputs": [],
      "source": [
        "# classe della rete\n",
        "\n",
        "class ClassifierDeep(nn.Module):\n",
        "\n",
        "    def __init__(self, labels, hdim, dropout):\n",
        "        super(ClassifierDeep, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hdim, hdim),\n",
        "            nn.BatchNorm1d(hdim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hdim, labels),\n",
        "            )\n",
        "\n",
        "    def forward(self, input_texts):\n",
        "        return self.classifier(input_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In8JtAI6u-FE"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta              # valore minimo di decrescita della loss di validazione all'epoca corrente\n",
        "                                                # per asserire che c'è un miglioramenti della loss\n",
        "        self.counter = 0                        # contatore delle epoche di pazienza\n",
        "        self.early_stop = False                 # flag di early stop\n",
        "        self.min_validation_loss = torch.inf    # valore corrente ottimo della loss di validazione\n",
        "\n",
        "    def __call__(self, validation_loss):\n",
        "        # chiamata in forma funzionale dell'oggetto di classe EarlySopping\n",
        "\n",
        "        if (validation_loss + self.min_delta) >= self.min_validation_loss:  # la loss di validazione non decresce\n",
        "            self.counter += 1                                               # incrementiamo il contatore delle epoche di pazienza\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(\"Early stop!\")\n",
        "        else:                                               # c'è un miglioramento della loss:\n",
        "            self.min_validation_loss = validation_loss      # consideriamo la loss corrente\n",
        "                                                            # come nuova loss ottimale\n",
        "            self.counter = 0                                # e azzeriamo il contatore di pazienza\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HELuTg_mFiTQ"
      },
      "outputs": [],
      "source": [
        "def gen_embeddings(input_id_text, attention_mask, lm_model):\n",
        "    with torch.no_grad():\n",
        "        last_hidden_states = lm_model(input_id_text, attention_mask=attention_mask).last_hidden_state\n",
        "        last_hidden_states = last_hidden_states[:,0,:]\n",
        "    return last_hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5jS3O-VvHv_"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, dataloader, tokenizer, lm_model, loss, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_texts, batch_labels in tqdm(dataloader, desc='training set'):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tokens = tokenizer(list(batch_texts), add_special_tokens=True, return_tensors='pt', padding='max_length', max_length = 512, truncation=True)\n",
        "        input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
        "        mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        embeddings_texts = gen_embeddings(input_id_texts, mask_texts, lm_model)\n",
        "        output = model(embeddings_texts)\n",
        "\n",
        "        # la loss è una CrossEntropyLoss, al suo interno ha\n",
        "        # la logsoftmax + negative log likelihood loss\n",
        "        batch_loss = loss(output, batch_labels)\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += batch_loss.item()\n",
        "\n",
        "        # per calcolare l'accuracy devo generare le predizioni\n",
        "        # applicando manualmente la logsoftmax\n",
        "        softmax = nn.LogSoftmax(dim=1)\n",
        "        epoch_acc += (softmax(output).argmax(dim=1) == batch_labels).sum().item()\n",
        "\n",
        "        batch_labels = batch_labels.detach().cpu()\n",
        "        embeddings_texts = embeddings_texts.detach().cpu()\n",
        "        output = output.detach().cpu()\n",
        "\n",
        "    return epoch_loss/len(dataloader), epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atbTc9ex1Q7S"
      },
      "outputs": [],
      "source": [
        "def test_loop(model, dataloader, tokenizer, lm_model, loss, device):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_texts, batch_labels, in tqdm(dataloader, desc='dev set'):\n",
        "\n",
        "            tokens = tokenizer(list(batch_texts), add_special_tokens=True,\n",
        "        return_tensors='pt', padding='max_length', max_length = 512, truncation=True)\n",
        "            input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
        "            mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            embeddings_texts = gen_embeddings(input_id_texts, mask_texts, lm_model)\n",
        "            output = model(embeddings_texts)\n",
        "\n",
        "            # la loss è una CrossEntropyLoss, al suo interno ha\n",
        "            # la logsoftmax + negative log likelihood loss\n",
        "            batch_loss = loss(output, batch_labels)\n",
        "            epoch_loss += batch_loss.item()\n",
        "\n",
        "            # per calcolare l'accuracy devo generare le predizioni\n",
        "            # applicando manualmente la logsoftmax\n",
        "            softmax = nn.LogSoftmax(dim=1)\n",
        "            epoch_acc += (softmax(output).argmax(dim=1) == batch_labels).sum().item()\n",
        "\n",
        "            batch_labels = batch_labels.detach().cpu()\n",
        "            embeddings_texts = embeddings_texts.detach().cpu()\n",
        "            output = output.detach().cpu()\n",
        "\n",
        "    return epoch_loss/len(dataloader), epoch_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK_cnWEHMavf"
      },
      "outputs": [],
      "source": [
        "def train_test(model, epochs, optimizer, device, train_data, test_data,\n",
        "               batch_size, model_name, train_loss_fn,\n",
        "               test_loss_fn=None,         # non necessariamente train e test loss devono differire\n",
        "               early_stopping=None,       # posso addstrare senza early stopping\n",
        "               val_data=None,             # e in questo caso non c'è validation set\n",
        "               scheduler=None):           # possibile scheduler per monitorare l'andamento di un iperparametro,\n",
        "                                          # tipicamente il learning rate\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    # check sulle funzioni di loss\n",
        "    if test_loss_fn == None:\n",
        "        test_loss_fn = train_loss_fn\n",
        "\n",
        "    # liste dei valori di loss e accuracy epoca per epoca per il plot\n",
        "    train_loss = []\n",
        "    validation_loss = []\n",
        "    test_loss = []\n",
        "\n",
        "    train_acc = []\n",
        "    validation_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    lm_model = AutoModel.from_pretrained(model_name, config=config).to(device)\n",
        "\n",
        "    # Ciclo di addestramento con early stopping\n",
        "    for epoch in tqdm(range(1,epochs+1)):\n",
        "\n",
        "        epoch_train_loss, epoch_train_acc = train_loop(model, train_dataloader, tokenizer, lm_model, train_loss_fn, optimizer, device)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        train_acc.append(epoch_train_acc/len(train_data))\n",
        "\n",
        "        # validation se è presente la callback di early stopping\n",
        "        if early_stopping != None:\n",
        "                epoch_validate_loss, epoch_validate_acc = test_loop(model,\n",
        "                val_dataloader, tokenizer, lm_model, test_loss_fn, device)\n",
        "                validation_loss.append(epoch_validate_loss)\n",
        "                validation_acc.append(epoch_validate_acc/len(val_data))\n",
        "\n",
        "        # test\n",
        "        epoch_test_loss, epoch_test_acc,= test_loop(model, test_dataloader, tokenizer, lm_model, test_loss_fn, device)\n",
        "        test_loss.append(epoch_test_loss)\n",
        "        test_acc.append(epoch_test_acc/len(test_data))\n",
        "\n",
        "        val_loss_str = f'Validation loss: {epoch_validate_loss:6.4f} ' if early_stopping != None else ' '\n",
        "        val_acc_str = f'Validation accuracy: {(epoch_validate_acc/len(val_data)):6.4f} ' if early_stopping != None else ' '\n",
        "        print(f\"\\nTrain loss: {epoch_train_loss:6.4f} {val_loss_str} Test loss: {epoch_test_loss:6.4f}\")\n",
        "        print(f\"Train accuracy: {(epoch_train_acc/len(train_data)):6.4f} {val_acc_str}Test accuracy: {(epoch_test_acc/len(test_data)):6.4f}\")\n",
        "\n",
        "        # early stopping\n",
        "        if early_stopping != None:\n",
        "                early_stopping(epoch_validate_loss)\n",
        "                if early_stopping.early_stop:\n",
        "                    break\n",
        "\n",
        "    return train_loss, validation_loss, test_loss, train_acc, validation_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWgCYjxPPD8i"
      },
      "outputs": [],
      "source": [
        "# Acquisiamo il device su cui effettueremo il training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "model = ClassifierDeep(len(labels_dict), hyperparameters[\"h_dim\"], hyperparameters[\"dropout\"]).to(device)\n",
        "print(model)\n",
        "\n",
        "# Calcoliamo il numero totale dei parametri del modello\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Numbero totale dei parametri: {total_params}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "\n",
        "# Creiamo la callback di early stopping da passare al nostro metodo di addestramento\n",
        "early_stopping = EarlyStopping(patience=hyperparameters['patience'], min_delta=hyperparameters['min_delta'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bEdv8U-wAL3L",
        "outputId": "2950c670-327d-4a12-e53d-fbc1c94430c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:01<00:47,  1.88s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:44,  1.84s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:42,  1.83s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:40,  1.82s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:38,  1.84s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:37,  1.86s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:35,  1.88s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:14<00:33,  1.87s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:16<00:31,  1.86s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:18<00:29,  1.85s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:20<00:27,  1.85s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:22<00:25,  1.84s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:24<00:24,  1.86s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:26<00:22,  1.89s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:27<00:20,  1.88s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:29<00:18,  1.87s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:31<00:16,  1.86s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:33<00:14,  1.85s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:35<00:12,  1.85s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:37<00:11,  1.88s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:39<00:09,  1.90s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:41<00:07,  1.88s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:42<00:05,  1.88s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:44<00:03,  1.87s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:46<00:00,  1.80s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.85s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.90s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.75s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:01<00:11,  1.96s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:03<00:09,  1.92s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:05<00:07,  1.90s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:07<00:05,  1.91s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.90s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.89s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.88s/it]\n",
            " 20%|██        | 1/5 [01:05<04:20, 65.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss: 0.0616 Validation loss: 0.0908  Test loss: 0.0737\n",
            "Train accuracy: 0.9850 Validation accuracy: 0.9775 Test accuracy: 0.9730\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:02<00:50,  2.00s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:46,  1.94s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:44,  1.93s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:42,  1.92s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:40,  1.91s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:38,  1.91s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:36,  1.93s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:15<00:35,  1.95s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:17<00:32,  1.93s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:19<00:30,  1.92s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:21<00:28,  1.91s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:23<00:26,  1.92s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:25<00:24,  1.92s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:26<00:23,  1.93s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:28<00:21,  1.95s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:30<00:19,  1.93s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:32<00:17,  1.92s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:34<00:15,  1.91s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:36<00:13,  1.91s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:38<00:11,  1.91s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:40<00:09,  1.93s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:42<00:07,  1.96s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:44<00:05,  1.95s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:46<00:03,  1.94s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:48<00:00,  1.86s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.91s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.92s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:02<00:12,  2.03s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:03<00:09,  1.96s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:05<00:07,  1.94s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:07<00:05,  1.93s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.93s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.92s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.92s/it]\n",
            " 40%|████      | 2/5 [02:12<03:18, 66.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss: 0.0326 Validation loss: 0.0843  Test loss: 0.0721\n",
            "Train accuracy: 0.9925 Validation accuracy: 0.9775 Test accuracy: 0.9820\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:02<00:50,  2.02s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:47,  1.96s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:44,  1.94s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:42,  1.93s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:40,  1.93s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:38,  1.93s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:37,  1.95s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:15<00:35,  1.97s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:17<00:33,  1.95s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:19<00:31,  1.95s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:21<00:29,  1.94s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:23<00:27,  1.94s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:25<00:25,  1.94s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:27<00:23,  1.95s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:29<00:21,  1.98s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:31<00:19,  1.96s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:33<00:17,  1.96s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:35<00:15,  1.96s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:37<00:13,  1.95s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:39<00:11,  1.95s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:41<00:09,  1.96s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:43<00:07,  1.98s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:44<00:05,  1.97s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:46<00:03,  1.96s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:48<00:00,  1.88s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.94s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.94s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:02<00:12,  2.01s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:03<00:09,  1.97s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:05<00:07,  1.96s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:07<00:05,  1.96s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.95s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.95s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.94s/it]\n",
            " 60%|██████    | 3/5 [03:20<02:14, 67.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss: 0.0190 Validation loss: 0.0852  Test loss: 0.0659\n",
            "Train accuracy: 0.9975 Validation accuracy: 0.9719 Test accuracy: 0.9798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:02<00:50,  2.02s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:47,  1.97s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:44,  1.95s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:42,  1.95s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:40,  1.95s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:39,  1.95s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:37,  1.98s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:15<00:35,  2.00s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:17<00:33,  1.98s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:19<00:31,  1.98s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:21<00:29,  1.97s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:23<00:27,  1.97s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:25<00:25,  1.97s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:27<00:23,  1.99s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:29<00:22,  2.01s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:31<00:19,  1.99s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:33<00:17,  1.98s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:35<00:15,  1.97s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:37<00:13,  1.97s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:39<00:11,  1.97s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:41<00:09,  1.98s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:43<00:07,  2.00s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:45<00:05,  1.99s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:47<00:03,  1.98s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:49<00:00,  1.91s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.95s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.96s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.81s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:02<00:12,  2.03s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:03<00:09,  1.99s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:05<00:07,  1.97s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:07<00:05,  1.98s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.97s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.96s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.95s/it]\n",
            " 80%|████████  | 4/5 [04:28<01:07, 67.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss: 0.0101 Validation loss: 0.0994  Test loss: 0.0726\n",
            "Train accuracy: 1.0000 Validation accuracy: 0.9719 Test accuracy: 0.9798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "training set:   0%|          | 0/26 [00:00<?, ?it/s]\u001b[A\n",
            "training set:   4%|▍         | 1/26 [00:02<00:50,  2.02s/it]\u001b[A\n",
            "training set:   8%|▊         | 2/26 [00:03<00:47,  1.99s/it]\u001b[A\n",
            "training set:  12%|█▏        | 3/26 [00:05<00:45,  1.98s/it]\u001b[A\n",
            "training set:  15%|█▌        | 4/26 [00:07<00:43,  1.98s/it]\u001b[A\n",
            "training set:  19%|█▉        | 5/26 [00:09<00:41,  1.98s/it]\u001b[A\n",
            "training set:  23%|██▎       | 6/26 [00:11<00:39,  1.98s/it]\u001b[A\n",
            "training set:  27%|██▋       | 7/26 [00:13<00:38,  2.01s/it]\u001b[A\n",
            "training set:  31%|███       | 8/26 [00:15<00:36,  2.01s/it]\u001b[A\n",
            "training set:  35%|███▍      | 9/26 [00:17<00:33,  2.00s/it]\u001b[A\n",
            "training set:  38%|███▊      | 10/26 [00:19<00:31,  1.99s/it]\u001b[A\n",
            "training set:  42%|████▏     | 11/26 [00:21<00:29,  1.98s/it]\u001b[A\n",
            "training set:  46%|████▌     | 12/26 [00:23<00:27,  1.98s/it]\u001b[A\n",
            "training set:  50%|█████     | 13/26 [00:25<00:25,  1.98s/it]\u001b[A\n",
            "training set:  54%|█████▍    | 14/26 [00:27<00:23,  2.00s/it]\u001b[A\n",
            "training set:  58%|█████▊    | 15/26 [00:29<00:22,  2.01s/it]\u001b[A\n",
            "training set:  62%|██████▏   | 16/26 [00:31<00:19,  2.00s/it]\u001b[A\n",
            "training set:  65%|██████▌   | 17/26 [00:33<00:17,  1.99s/it]\u001b[A\n",
            "training set:  69%|██████▉   | 18/26 [00:35<00:15,  1.99s/it]\u001b[A\n",
            "training set:  73%|███████▎  | 19/26 [00:37<00:13,  1.99s/it]\u001b[A\n",
            "training set:  77%|███████▋  | 20/26 [00:39<00:11,  1.98s/it]\u001b[A\n",
            "training set:  81%|████████  | 21/26 [00:41<00:10,  2.02s/it]\u001b[A\n",
            "training set:  85%|████████▍ | 22/26 [00:43<00:08,  2.01s/it]\u001b[A\n",
            "training set:  88%|████████▊ | 23/26 [00:45<00:06,  2.00s/it]\u001b[A\n",
            "training set:  92%|█████████▏| 24/26 [00:47<00:03,  1.99s/it]\u001b[A\n",
            "training set: 100%|██████████| 26/26 [00:49<00:00,  1.92s/it]\n",
            "\n",
            "dev set:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  33%|███▎      | 1/3 [00:01<00:03,  1.98s/it]\u001b[A\n",
            "dev set:  67%|██████▋   | 2/3 [00:03<00:01,  1.99s/it]\u001b[A\n",
            "dev set: 100%|██████████| 3/3 [00:05<00:00,  1.83s/it]\n",
            "\n",
            "dev set:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "dev set:  14%|█▍        | 1/7 [00:02<00:12,  2.06s/it]\u001b[A\n",
            "dev set:  29%|██▊       | 2/7 [00:04<00:10,  2.01s/it]\u001b[A\n",
            "dev set:  43%|████▎     | 3/7 [00:06<00:07,  1.99s/it]\u001b[A\n",
            "dev set:  57%|█████▋    | 4/7 [00:08<00:05,  1.99s/it]\u001b[A\n",
            "dev set:  71%|███████▏  | 5/7 [00:09<00:03,  1.99s/it]\u001b[A\n",
            "dev set:  86%|████████▌ | 6/7 [00:11<00:01,  1.98s/it]\u001b[A\n",
            "dev set: 100%|██████████| 7/7 [00:13<00:00,  1.98s/it]\n",
            "100%|██████████| 5/5 [05:38<00:00, 67.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train loss: 0.0086 Validation loss: 0.0906  Test loss: 0.0676\n",
            "Train accuracy: 0.9994 Validation accuracy: 0.9719 Test accuracy: 0.9775\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Routine di addestramento\n",
        "train_loss, validation_loss,test_loss,\n",
        "train_acc, validation_acc, test_acc = train_test(model, hyperparameters['epochs'], optimizer, device, train_dataset,\n",
        "test_dataset, hyperparameters['batch_size'], hyperparameters['language_model'], criterion, criterion, early_stopping, val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3jFVtjAzFqO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "axs[0].plot(train_loss, label='training loss')\n",
        "axs[0].plot(validation_loss, label='validation loss')\n",
        "axs[0].plot(test_loss, label='test loss')\n",
        "axs[0].legend(loc='upper right')\n",
        "axs[0].set_ylim(0,1)\n",
        "\n",
        "axs[1].plot(train_acc, label='training accuracy')\n",
        "axs[1].plot(validation_acc, label='validation accuracy')\n",
        "axs[1].plot(test_acc, label='test accuracy')\n",
        "axs[1].legend(loc='lower right')\n",
        "axs[1].set_ylim(0,1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
