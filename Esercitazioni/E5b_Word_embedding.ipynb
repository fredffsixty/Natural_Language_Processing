{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analisi di testi con GloVe e Word2vec\n",
        "\n",
        "Inizieremo da GloVe che può essere scaricato dal relativo repository GitHub. Useremo una versione piccola e anche non molto aggiornata degli embedding per motivi di spazio.\n",
        "\n",
        "```bash\n",
        "$ curl -o <Cartella di destinazione>/glove.6B.zip https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
        "$ cd <Cartella di destinazione>\n",
        "$ unzip glove.6B.zip\n",
        "```\n",
        "\n",
        "Questo file contiene un archivio di circa 822MB e va estratto dall'archivio ottenendo il file ```glove.6B.300d.txt``` che ha una dimensione di oltre 1GB. \n",
        "\n",
        "Lavorando su Google Colab, il file va uploadato su Drive e il Drive va montato nel runtime. \n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "```\n",
        "\n",
        "Se non si dispone di spazio sufficiente sul proprio Drive allora è opportuno che, una volta creato il notebook e connesso il relativo runtime, si apra l'icona del filesytem e, all'interno della cartella ```/content``` si faccia il drag & drop del file stesso in modo da usare l'ampio spazio fornito direttamente dal runtime. \n",
        "\n",
        "Ovviamente alla fine della sessione si perderà tutto il contenuto e si dovrà ripetere il processo in caso che il file debba essere usato nuovamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAQwQ6e-LM2G"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import nltk\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mA2SVdVH8Jr"
      },
      "outputs": [],
      "source": [
        "root = \"/home/rpirrone/src/\"     # Da personalizzare con il proprio percorso che contiene gli embedding\n",
        "\n",
        "\n",
        "def load_glove(glove_path):\n",
        "\n",
        "    print(\"Loading glove vectors ...\")\n",
        "    with open(glove_path, encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
        "        glove_embeddings = {line[0]: np.array(list(map(float, line[1:])))\n",
        "                for line in reader}\n",
        "    print(\"Glove vectors loaded\")\n",
        "    return glove_embeddings\n",
        "\n",
        "glove_embeddings = load_glove(root+\"glove.6B.300d.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jDQUdUBuQ4Z",
        "outputId": "4eb4c53b-7d81-4b25-ba45-608405ef6702"
      },
      "outputs": [],
      "source": [
        "# Analizziamo un attimo il corpus\n",
        "print(f\"{len(glove_embeddings)} vettori da {glove_embeddings['cat'].shape[0]} elementi ciascuno\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fxHGrwaN8bk",
        "outputId": "3a8389e9-b71a-43b4-d823-26f8242dc36e"
      },
      "outputs": [],
      "source": [
        "## To find the nearest neighbors of a word\n",
        "def find_nearest(word, glove_embeddings, k=5):\n",
        "  \n",
        "  distances = []\n",
        "  word_vec = glove_embeddings[word]\n",
        "  \n",
        "  for w, vec in glove_embeddings.items():\n",
        "    distance = np.linalg.norm(word_vec - vec)\n",
        "    distances.append((w, distance))\n",
        "  distances = sorted(distances, key=lambda x: x[1])\n",
        "  \n",
        "  return distances[:k]\n",
        "\n",
        "print(find_nearest('cat', glove_embeddings))\n",
        "print(find_nearest('water', glove_embeddings))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OivhUiQoOKkb",
        "outputId": "090d7a96-2d90-4857-ac5d-ce47355c41b6"
      },
      "outputs": [],
      "source": [
        "## Calcoliamo le analogie tra parole con la regola del parallelogramma\n",
        "def find_analogy(a, b, c, glove_embeddings):\n",
        "  a_vec = glove_embeddings[a]\n",
        "  b_vec = glove_embeddings[b]\n",
        "  c_vec = glove_embeddings[c]\n",
        "  d_vec = b_vec - a_vec + c_vec\n",
        "  distances = []\n",
        "  \n",
        "  for w, vec in glove_embeddings.items():\n",
        "    distance = np.linalg.norm(d_vec - vec)  \n",
        "    distances.append((w, distance))\n",
        "  distances = sorted(distances, key=lambda x: x[1])\n",
        "  \n",
        "  return distances[:1][0] # restituiamo direttamente la coppia (parola, distanza dal valore ideale)\n",
        "\n",
        "word, distance = find_analogy('king', 'man', 'queen', glove_embeddings)\n",
        "print(f\"'king' : 'man' --> 'queen' : '{word}'. Distanza: {distance}\\n\\n\")\n",
        "\n",
        "word, distance = find_analogy('paris', 'france', 'rome', glove_embeddings)\n",
        "print(f\"'paris' : 'france' --> 'rome' : '{word}'. Distanza: {distance}\\n\\n\")\n",
        "\n",
        "word, distance = find_analogy('woman', 'actress', 'man', glove_embeddings)\n",
        "print(f\"'woman' : 'actress' --> 'man' : '{word}'. Distanza: {distance}\\n\\n\")\n",
        "\n",
        "word, distance = find_analogy('pianist', 'piano', 'guitarist', glove_embeddings)\n",
        "print(f\"'pianist' : 'piano' --> 'guitarist' : '{word}'. Distanza: {distance}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizziamo gli embedding in uno spazio bidimensionale usando l'algoritmo t-SNE per la riduzione della dimensionalità. t-SNE converte le somiglianze tra punti dati in probabilità congiunte e cerca di minimizzare la divergenza di Kullback-Leibler tra le probabilità congiunte dell’embedding a bassa dimensionalità e quelle dei dati ad alta dimensionalità.\n",
        "\n",
        "La funzione di costo di t-SNE non è convessa, cioè con diverse inizializzazioni si possono ottenere risultati differenti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "tDXTeA86R-cX",
        "outputId": "1be54b9c-928c-4a76-ad24-fa421af281c7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "words = ['woman', 'actress', 'man', 'actor', 'pianist',\n",
        "         'piano', 'guitarist', 'guitar', 'king', 'queen']\n",
        "\n",
        "def visualize_embeddings(embeddings, words):\n",
        "    \n",
        "    pca = PCA(n_components=len(words),svd_solver='full')\n",
        "    tsne = TSNE(n_components=2, random_state=0, perplexity=len(words)-1)\n",
        "    \n",
        "    embedding_vectors = np.array([embeddings[word] for word in words])\n",
        "    pca_embeddings = pca.fit_transform(embedding_vectors)\n",
        "    two_d_embeddings = tsne.fit_transform(pca_embeddings)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    for i, word in enumerate(words):\n",
        "        x, y = two_d_embeddings[i, :]\n",
        "        plt.scatter(x, y)\n",
        "        plt.annotate(word, (x, y), xytext=(5, 2),\n",
        "                     textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\n",
        "    plt.show()\n",
        "\n",
        "glove_words = [word for word in words if word in glove_embeddings]\n",
        "visualize_embeddings(glove_embeddings, glove_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9MXvIt_LUzr",
        "outputId": "6c131126-e137-47f3-c01a-dbee2ec36407"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import brown\n",
        "from pprint import pprint\n",
        "\n",
        "# Scarichiamo il Brown corpus usando NLTK\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/brown')\n",
        "except LookupError:\n",
        "    nltk.download('brown')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"ANALISI DEL CORPUS BROWN\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "categories = brown.categories()\n",
        "words_brown = brown.words(categories=categories)\n",
        "sentences = brown.sents()\n",
        "\n",
        "print(f\"{len(categories)} categorie:\\n\\n\")\n",
        "pprint(categories)\n",
        "\n",
        "print(f\"\\n\\n{len(sentences)} frasi:\\n\\nEsempi:\\n\")\n",
        "for sentence in sentences[:2]:\n",
        "    print(sentence)\n",
        "\n",
        "print(f\"{len(words_brown)} parole.\\n\\n\")\n",
        "\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "gDEpisvUS2Az",
        "outputId": "5f6981d5-040f-49ba-d185-44db7d7d0760"
      },
      "outputs": [],
      "source": [
        "glove_words = []\n",
        "for sent in brown.sents()[:4]:\n",
        "    glove_words.extend([word for word in sent if word in glove_embeddings])\n",
        "visualize_embeddings(glove_embeddings, glove_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wrd2vec lo prendiamo da Gensim, https://radimrehurek.com/gensim/index.html, che è una libreria open source Python per addestrare e gestire word embedding differenti."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "# Creiamo un training set e addestriamo il modello Word2vec della libreria Gensim\n",
        "train_set = brown.sents()[:10000]\n",
        "model = gensim.models.Word2Vec(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salviamo e carichiamo il modello\n",
        "model.save('brown.embedding')\n",
        "new_model = gensim.models.Word2Vec.load('brown.embedding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stampiamo le caratteristiche del modello appreso\n",
        "print(f\"{len(new_model.wv)} embedding da {len(new_model.wv['university'])} elementi ciascuno.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcolo della 5 parole più vicine alla lista di parole già usata per GloVe\n",
        "words = ['university', 'school', 'jury', 'investigation', 'movie']\n",
        "for word in words:\n",
        "    print(f\"\\n\\nLe cinque parole più simili a '{word}' sono:\\n\")\n",
        "    pprint(new_model.wv.most_similar(positive=[word],topn=5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizziamo gli embedding Word2vec\n",
        "    \n",
        "w2v_words = [word for word in words if word in new_model.wv]\n",
        "visualize_embeddings(new_model.wv, w2v_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Il modello che abbiamo usato è addestrato su pochi termini. Carichiamone uno pre-addestrato per effettuare il ragionamento per analogia. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    nltk.data.find('models/word2vec_sample')\n",
        "except LookupError:\n",
        "    nltk.download('word2vec_sample')\n",
        "\n",
        "word2vec_sample = str(nltk.data.find('models/word2vec_sample/pruned.word2vec.txt')) # otteniamo il path\n",
        "\n",
        "# carichiamo il modello preaddestrato che è in un formato diverso\n",
        "# per cui si accede direttamente dal modello senza la proprietà 'wv'\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Ragionamento per analogia\n",
        "word, similarita = model.most_similar(positive=['woman','king'], negative=['man'], topn = 1)[0]\n",
        "\n",
        "print(f\"'woman' : 'king' --> 'man' : '{word}'. Similarità: {similarita}\\n\\n\")\n",
        "\n",
        "word, similarita = model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 1)[0]\n",
        "print(f\"'Paris' : 'Germany' --> 'Berlin' : '{word}'. Distanza: {similarita}\\n\\n\")\n",
        "\n",
        "word, similarita = model.most_similar(positive=['woman','actor'], negative=['man'], topn = 1)[0]\n",
        "print(f\"'woman' : 'actor' --> 'man' : '{word}'. Distanza: {similarita}\\n\\n\")\n",
        "\n",
        "word, similarita = model.most_similar(positive=['guitarist','piano'], negative=['pianist'], topn = 1)[0]\n",
        "print(f\"'guitarist' : 'piano' --> 'pianist' : '{word}'. Distanza: {similarita}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizziamo gli embedding Word2vec\n",
        "\n",
        "words = ['man', 'woman', 'king', 'queen', 'Pairs', 'Berlin', 'France', 'Germany', 'actor', \n",
        "         'actress', 'pianist', 'guitarist', 'piano', 'guitar']\n",
        "    \n",
        "w2v_words = [word for word in words if word in model]\n",
        "visualize_embeddings(model, w2v_words)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
