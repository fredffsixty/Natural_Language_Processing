{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "510787ec81d348f5b7aa21ad1bf77f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21988e42f77e45119be23c95c1f7bcd3",
              "IPY_MODEL_681ef8c65750471f92e3f3e6b1e5369d",
              "IPY_MODEL_08182a5d95124b18b9c483b41650f098"
            ],
            "layout": "IPY_MODEL_c3126dfd4a674be2ae4f2cab7c22942d"
          }
        },
        "21988e42f77e45119be23c95c1f7bcd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e6205c41e7a4f6cba43b0c0889b0f9a",
            "placeholder": "​",
            "style": "IPY_MODEL_5631d8b49b7947d4a4e799ac03614e6b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "681ef8c65750471f92e3f3e6b1e5369d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_249fc6f7d32b4516afa9b8049e7cc26e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f63acaa23c440809e2d2047dca25ccf",
            "value": 2
          }
        },
        "08182a5d95124b18b9c483b41650f098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce27162d580c4d6c8eb76261c99a02c8",
            "placeholder": "​",
            "style": "IPY_MODEL_15a3df8fbeb043ccb35dbf7244de0a62",
            "value": " 2/2 [01:18&lt;00:00, 35.54s/it]"
          }
        },
        "c3126dfd4a674be2ae4f2cab7c22942d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e6205c41e7a4f6cba43b0c0889b0f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5631d8b49b7947d4a4e799ac03614e6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "249fc6f7d32b4516afa9b8049e7cc26e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f63acaa23c440809e2d2047dca25ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce27162d580c4d6c8eb76261c99a02c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a3df8fbeb043ccb35dbf7244de0a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Articoli per utleriori spiegazioni:\n",
        "\n",
        "https://medium.com/@kshitiz.sahay26/fine-tuning-llama-2-for-news-category-prediction-a-step-by-step-comprehensive-guide-to-fine-tuning-48c06dee28a9\n",
        "\n",
        "https://medium.com/@jain.sm/finetuning-llama-2-0-on-colab-with-1-gpu-7ea73a8d3db9\n"
      ],
      "metadata": {
        "id": "NVhvpda_S_3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Required Libraries\n",
        "\n",
        "First, we will install some required libraries.\n",
        "\n",
        "`transformers`: for loading a large language model and fine-tuning it.\n",
        "\n",
        "`bitsandbytes`: for loading the model in 4-bit precision.\n",
        "\n",
        "`accelerate`: for training models and performing inference at scale.\n",
        "\n",
        "`peft`: for fine-tuning a small number of parameters.\n",
        "\n",
        "`trl`: for training transformer language models using Reinforcement Learning."
      ],
      "metadata": {
        "id": "VOLbbZXRTSI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "# 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\n",
        "# and insert your token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhBfyV5zTRi7",
        "outputId": "cbc88bff-d720-4223-bcad-bdd25bfb98a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZUcQt4mS5O4"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 --progress-bar off\n",
        "!pip install -q peft==0.4.0 --progress-bar off\n",
        "!pip install -q bitsandbytes==0.40.2 --progress-bar off\n",
        "!pip install -q transformers==4.31.0 --progress-bar off\n",
        "!pip install -q trl==0.4.7 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from random import randrange\n",
        "from functools import partial\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          HfArgumentParser,\n",
        "                          Trainer,\n",
        "                          TrainingArguments,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          EarlyStoppingCallback,\n",
        "                          pipeline,\n",
        "                          logging,\n",
        "                          set_seed)\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel, AutoPeftModelForCausalLM\n",
        "from trl import SFTTrainer\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbBiJWPfTLl5",
        "outputId": "598d76b3-ccf9-45ec-b7cd-1cd84e1cf977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face Hub Login\n",
        "\n",
        "Meta's family of Llama 2 models is gated. You will require approval to access it using the Hugging Face Hub.\n",
        "\n",
        "Below are the steps to request permission for the Llama-2-7B model:\n",
        "1. Get approval from Hugging Face (https://huggingface.co/meta-llama/Llama-2-7b-hf).\n",
        "2. Get approval from Meta (https://ai.meta.com/resources/models-and-libraries/llama-downloads/).\n",
        "3. Create a READ access token on Hugging Face (https://huggingface.co/settings/tokens).\n",
        "4. Execute `!huggingface-cli login` in Google Colab Notebook, enter the token, and enter \"Y.\"\n",
        "\n",
        "Note: Make sure your email address on your Hugging Face account is the same as the one you enter on Meta's website for approval."
      ],
      "metadata": {
        "id": "ehr_G7p7TfXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Bitsandbytes Configuration\n",
        "\n",
        "Before loading the model, we will define a function `create_bnb_config` to define the `bitsandbytes` configuration. The `bitsandbytes` library allows model quantization. Quantization is a technique used to compress deep learning models by reducing the number of bits used to represent their weights and activations. This compression allows for faster inference and reduced memory consumption, making it possible to deploy these models on edge devices with limited resources.\n",
        "\n",
        "By using 4-bit transformer language models, we can achieve impressive results while significantly reducing memory and computational requirements.\n",
        "\n",
        "Hugging Face Transformers (`transformers`) is closely integrated with `bitsandbytes`. The `BitsAndBytesConfig` class from the `transformers` library allows configuring the model quantization method.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "`load_in_4bit`: Load the model in 4-bit precision, i.e., divide memory usage by 4.\n",
        "\n",
        "`bnb_4bit_use_double_quant`: Use nested quantization techniques for more memory-efficient inference at no additional cost.\n",
        "\n",
        "`bnb_4bit_quant_type`: Set quantization data type. The options are either FP4 (4-bit precision), which is the default quantization data type, or NF4 (Normal Float 4), a new 4-bit data type adapted for weights that have been initialized using a normal distribution.\n",
        "\n",
        "`bnb_4bit_compute_dtype`: Set the computational data type for 4-bit models. Default value: torch.float32"
      ],
      "metadata": {
        "id": "P_aUroOTVm2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config_parameters = {\n",
        "    \"load_in_4bit\" : True, # Activate 4-bit precision base model loading\n",
        "    \"bnb_4bit_use_double_quant\" : True, # Activate nested quantization for 4-bit base models (double quantization)\n",
        "    \"bnb_4bit_quant_type\" : \"nf4\", # Quantization type (fp4 or nf4)\n",
        "    \"bnb_4bit_compute_dtype\" : torch.bfloat16 # Compute data type for 4-bit base models\n",
        "}\n",
        "\n",
        "def create_bnb_config(bnb_config_parameters):\n",
        "    \"\"\" Configures model quantization method using bitsandbytes to speed up training and inference\n",
        "    :param load_in_4bit: Load model in 4-bit precision mode\n",
        "    :param bnb_4bit_use_double_quant: Nested quantization for 4-bit model\n",
        "    :param bnb_4bit_quant_type: Quantization data type for 4-bit model\n",
        "    :param bnb_4bit_compute_dtype: Computation data type for 4-bit model \"\"\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit = bnb_config_parameters['load_in_4bit'],\n",
        "        bnb_4bit_use_double_quant = bnb_config_parameters['bnb_4bit_use_double_quant'],\n",
        "        bnb_4bit_quant_type = bnb_config_parameters['bnb_4bit_quant_type'],\n",
        "        bnb_4bit_compute_dtype = bnb_config_parameters['bnb_4bit_compute_dtype'],\n",
        "    )\n",
        "\n",
        "    return bnb_config"
      ],
      "metadata": {
        "id": "Chcu90U6TLdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Hugging Face Model and Tokenizer\n",
        "\n",
        "We will now define a function `load_model` that accepts the model name (`model_name`) from Hugging Face Hub and the `bitsandbytes` configuration for model quantization.\n",
        "\n",
        "In this function, we will perform the following steps:\n",
        " 1. Get the number of GPUs available.\n",
        " 2. Set the maximum GPU memory.\n",
        " 3. Use the from_pretrained` method from the `AutoModelForCausalLM` class to load a pre-trained Hugging Face model in 4-bit precision using the model name and the quantization configuration.\n",
        " 4. Set which device to send the model to using `device_map`. Passing `device_map = 0` means putting the whole model on GPU 0. Other inputs could be `cpu`, `cuda:1`, etc. Setting `device_map = auto` will let `accelerate` compute the most optimized `device_map` automatically.\n",
        " 5. Set `max_memory`, a dictionary device identifier, to maximum memory, which will default to the maximum memory available for each GPU and the available CPU RAM if unset.\n",
        " 6. Load the model tokenizer from the model name on Hugging Face.\n",
        " 7. Set a padding token to ensure shorter sequences will have the same length as the longest sequence in a batch. In this case, we will set the EOS (End of Sentence) token as the padding token.\n",
        "\n",
        " **Important Note:  A tokenizer for a model will preprocess and tokenize (convert letters/words/sub-words to tokens or numbers) the input in a way that the model expects. Model tokenizers are also responsible for correctly applying special tokens and certain special embeddings or positional encoders specific to a model in the input.**"
      ],
      "metadata": {
        "id": "J-LJIadEXc2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name, bnb_config):\n",
        "    \"\"\" Loads model and model tokenizer\n",
        "    :param model_name: Hugging Face model name\n",
        "    :param bnb_config: Bitsandbytes configuration \"\"\"\n",
        "\n",
        "    # Get number of GPU device and set maximum memory\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    max_memory = f'{40960}MB'\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config = bnb_config,\n",
        "        device_map = \"auto\", # dispatch the model efficiently on the available resources\n",
        "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
        "    )\n",
        "\n",
        "    # Load model tokenizer with the user authentication token\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token = True)\n",
        "\n",
        "    # Set padding token as EOS token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "u3OfeOfgXfcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating PEFT Configuration\n",
        "\n",
        "\n",
        "Fine-tuning pretrained LLMs on downstream datasets results in huge performance gains when compared to using the pretrained LLMs out-of-the-box. However, as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!\n",
        "\n",
        "\n",
        "PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. It also helps in portability, wherein users can tune models using PEFT methods to get tiny checkpoints worth a few MB compared to the large checkpoints of full fine-tuning.\n",
        "\n",
        "\n",
        "**In short, PEFT approaches enable you to get performance comparable to full fine-tuning while only having a small number of trainable parameters.**\n",
        "\n",
        "\n",
        "Hugging Face provides the PEFT library, which provides the latest Parameter-Efficient Fine-tuning techniques seamlessly integrated with Hugging Face Transformers and Hugging Face Accelerate.\n",
        "\n",
        "\n",
        "There are several PEFT methods. In the next cell, we will use QLoRA, one of the latest methods that reduces the memory usage of LLM finetuning without performance tradeoffs, using the `LoraConfig` class from the `peft` library.\n",
        "\n",
        "\n",
        "QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen, and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training."
      ],
      "metadata": {
        "id": "KqI3v9Qecy1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config_parameters = {\n",
        "    'r' : 16,   # LoRA attention dimension\n",
        "    'lora_alpha' : 64,  # Alpha parameter for LoRA scaling\n",
        "    'lora_dropout' : 0.1,   # Dropout probability for LoRA layers\n",
        "    'bias' : 'none',    # Bias\n",
        "    'task_type' : 'CAUSAL_LM',  # Task type\n",
        "}\n",
        "\n",
        "def create_peft_config(lora_config_parameters, target_modules):\n",
        "    \"\"\" Creates Parameter-Efficient Fine-Tuning configuration for the model\n",
        "    :param r: LoRA attention dimension\n",
        "    :param lora_alpha: Alpha parameter for LoRA scaling\n",
        "    :param modules: Names of the modules to apply LoRA to\n",
        "    :param lora_dropout: Dropout Probability for LoRA layers\n",
        "    :param bias: Specifies if the bias parameters should be trained \"\"\"\n",
        "    config = LoraConfig(\n",
        "        r = lora_config_parameters['r'],\n",
        "        lora_alpha = lora_config_parameters['lora_alpha'],\n",
        "        target_modules = target_modules,\n",
        "        lora_dropout = lora_config_parameters['lora_dropout'],\n",
        "        bias = lora_config_parameters['bias'],\n",
        "        task_type = lora_config_parameters['task_type'],\n",
        "    )\n",
        "\n",
        "    return config"
      ],
      "metadata": {
        "id": "a3kvC8QAcwSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Modules for LoRA Application\n",
        "\n",
        "In the next cell, we will define the `find_all_linear_names` function to find the module to apply LoRA to. This function will get the module names from `model.named_modules()` and store it in a set to keep distinct module names."
      ],
      "metadata": {
        "id": "OOeYojEkdQSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_linear_names(model):\n",
        "    \"\"\" Find modules to apply LoRA to.\n",
        "    :param model: PEFT model \"\"\"\n",
        "\n",
        "    cls = bnb.nn.Linear4bit\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:\n",
        "        lora_module_names.remove('lm_head')\n",
        "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
        "    return list(lora_module_names)"
      ],
      "metadata": {
        "id": "-m1G-HgOdRWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Trainable Parameters\n",
        "\n",
        "We can use the `print_trainable_parameters` function to find out the number and percentage of trainable model parameters. This function will calculate the number of total parameters in `model.named_parameters()` and then those that would get updated."
      ],
      "metadata": {
        "id": "jDHm6Z3KdT1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model, use_4bit = False):\n",
        "    \"\"\" Prints the number of trainable parameters in the model.\n",
        "    :param model: PEFT model \"\"\"\n",
        "\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "\n",
        "    for _, param in model.named_parameters():\n",
        "        num_params = param.numel()\n",
        "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
        "            num_params = param.ds_numel\n",
        "        all_param += num_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "\n",
        "    if use_4bit:\n",
        "        trainable_params /= 2\n",
        "\n",
        "    print(\n",
        "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "lpYdRC7EdWKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning the Pre-trained Model\n",
        "\n",
        "We will create `fine_tune`, our final function, to wrap everything we have done so far and initiate the fine-tuning process. This function will perform the following model preprocessing operations to prepare it for training:\n",
        "\n",
        "\n",
        "1. Enable gradient checkpointing to reduce memory usage during fine-tuning.\n",
        "2. Use the `prepare_model_for_kbit_training` function from PEFT to prepare the model for fine-tuning.\n",
        "3. Call find_all_linear_names` to get the module names to apply LoRA to.\n",
        "4. Create LoRA configuration by calling the `create_peft_config` function.\n",
        "5. Wrap the base Hugging Face model for fine-tuning to PEFT using the `get_peft_model` function.\n",
        "6. Print the trainable parameters.\n",
        "\n",
        "\n",
        "For training, we will instantiate a `Trainer()` object within the `fine_tune` function. This class requires the model, preprocessed dataset, and training arguments, listed below.\n",
        "\n",
        "\n",
        "`per_device_train_batch_size`: The batch size per GPU/TPU/CPU for training.\n",
        "\n",
        "\n",
        "`gradient_accumulation_steps`: Number of update steps to accumulate the gradients for, before performing a backward/update pass.\n",
        "\n",
        "\n",
        "`warmup_steps`: Number of steps used for a linear warmup from 0 to `learning_rate`.\n",
        "\n",
        "\n",
        "`max_steps`: If set to a positive number, the total number of training steps to perform.\n",
        "\n",
        "\n",
        "`learning_rate`: The initial learning rate for Adam.\n",
        "\n",
        "\n",
        "`fp16`: Whether to use 16-bit (mixed) precision training (through NVIDIA apex) instead of 32-bit training.\n",
        "\n",
        "\n",
        "`logging_steps`: Number of update steps between two logs.\n",
        "\n",
        "\n",
        "`output_dir`: The output directory where the model predictions and checkpoints will be written.\n",
        "\n",
        "\n",
        "`optim`: The optimizer to use for training.\n",
        "\n",
        "\n",
        "Next, we will use the `train` method on the trainer` object to start the training and log and save the model metrics on the training dataset. Finally, we will save the model checkpoint (model weights, configuration file, and tokenizer) in the output directory and delete the model to free up memory. You can load the model for inference later using its saved checkpoint."
      ],
      "metadata": {
        "id": "rj9zuIimdf3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune(model, tokenizer, dataset_train, dataset_test,\n",
        "#def fine_tune(model, tokenizer, dataset_train,\n",
        "            lora_config_parameters, training_parameters):\n",
        "    \"\"\" Prepares and fine-tune the pre-trained model.\n",
        "    :param model: Pre-trained Hugging Face model\n",
        "    :param tokenizer: Model tokenizer\n",
        "    :param dataset: Preprocessed training dataset \"\"\"\n",
        "\n",
        "    # Enable gradient checkpointing to reduce memory usage during fine-tuning\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    # Prepare the model for training\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Get LoRA module names\n",
        "    target_modules = find_all_linear_names(model)\n",
        "\n",
        "    # Create PEFT configuration for these modules and wrap the model to PEFT\n",
        "    peft_config = create_peft_config(lora_config_parameters, target_modules)\n",
        "    model = get_peft_model(model, peft_config)\n",
        "\n",
        "    # Print information about the percentage of trainable parameters\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Training parameters\n",
        "    trainer = Trainer(\n",
        "        model = model,\n",
        "        train_dataset = dataset_train,\n",
        "        eval_dataset = dataset_test, # inseriamo e vediamo come performa il modello\n",
        "                                    # lo stiamo mettendo come \"validation set\"\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = training_parameters[\"per_device_train_batch_size\"],\n",
        "            gradient_accumulation_steps = training_parameters[\"gradient_accumulation_steps\"],\n",
        "            warmup_steps = training_parameters[\"warmup_steps\"],\n",
        "            #max_steps = training_parameters[\"max_steps\"],\n",
        "            num_train_epochs = training_parameters[\"max_steps\"],\n",
        "            learning_rate = training_parameters[\"learning_rate\"],\n",
        "            fp16 = training_parameters[\"fp16\"],\n",
        "            logging_steps = training_parameters[\"logging_steps\"],\n",
        "            output_dir = training_parameters[\"output_dir\"],\n",
        "            optim = training_parameters[\"optim\"],\n",
        "        ),\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
        "    )\n",
        "\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    do_train = True\n",
        "\n",
        "    # Launch training and log metrics\n",
        "    print(\"Training...\")\n",
        "\n",
        "    if do_train:\n",
        "        train_result = trainer.train()\n",
        "        metrics = train_result.metrics\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "        print(metrics)\n",
        "        print('-----')\n",
        "        trainer.log_metrics(\"eval\", metrics)\n",
        "        trainer.save_metrics(\"eval\", metrics)\n",
        "        print(metrics)\n",
        "\n",
        "    # Save model\n",
        "    print(\"Saving last checkpoint of the model...\")\n",
        "    os.makedirs(training_parameters[\"output_dir\"], exist_ok = True)\n",
        "    trainer.model.save_pretrained(training_parameters[\"output_dir\"])\n",
        "\n",
        "    # Free memory for merging weights\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XrFseV9Jdcyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_parameters = {\n",
        "    \"model_name\" : \"meta-llama/Llama-2-7b-hf\", # The pre-trained model from the Hugging Face Hub to load and fine-tune\n",
        "    \"output_dir\" : \"./results\", # Output directory where the model predictions and checkpoints will be stored\n",
        "    \"per_device_train_batch_size\" : 1, # Batch size per GPU for training\n",
        "    \"gradient_accumulation_steps\" : 4, # Number of update steps to accumulate the gradients for\n",
        "    \"learning_rate\" : 2e-4, # Initial learning rate (AdamW optimizer)\n",
        "    \"optim\" : \"paged_adamw_32bit\", # Optimizer to use\n",
        "    \"max_steps\" : 20, # Number of training steps (overrides num_train_epochs)\n",
        "    \"warmup_steps\" : 2, # Linear warmup steps from 0 to learning_rate\n",
        "    \"fp16\" : True, # Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "    \"logging_steps\" : 1, # Log every X updates steps\n",
        "}\n",
        "\n",
        "\n",
        "# Load model from Hugging Face Hub with model name and bitsandbytes configuration\n",
        "\n",
        "bnb_config = create_bnb_config(bnb_config_parameters)\n",
        "model, tokenizer = load_model(training_parameters['model_name'], bnb_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106,
          "referenced_widgets": [
            "510787ec81d348f5b7aa21ad1bf77f2a",
            "21988e42f77e45119be23c95c1f7bcd3",
            "681ef8c65750471f92e3f3e6b1e5369d",
            "08182a5d95124b18b9c483b41650f098",
            "c3126dfd4a674be2ae4f2cab7c22942d",
            "6e6205c41e7a4f6cba43b0c0889b0f9a",
            "5631d8b49b7947d4a4e799ac03614e6b",
            "249fc6f7d32b4516afa9b8049e7cc26e",
            "6f63acaa23c440809e2d2047dca25ccf",
            "ce27162d580c4d6c8eb76261c99a02c8",
            "15a3df8fbeb043ccb35dbf7244de0a62"
          ]
        },
        "id": "pwAe2vHkeDWg",
        "outputId": "fef6df14-3199-4f16-d4ee-01a41495e196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "510787ec81d348f5b7aa21ad1bf77f2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Dataset\n",
        "\n",
        "Now that we have loaded the Llama-2-7B model and its tokenizer, we will move on to loading our news classification instruction dataset from the previous blog as a Hugging Face `Datasets`.\n",
        "\n",
        "Firstly, we will initialize the path of the dataset. In this case, we have a CSV file that contains 99 records, or prompts. This dataset contains an `instruction` column containing the instruction to categorize a news article into 18 categories, an `input` column containing the news article, and an `output` column containing the actual news category for training.\n",
        "\n",
        "We will use the `load_dataset` function and pass the file location. We will define a generic dataset builder name `csv` because our dataset is a CSV file. You can similarly load a JSON file by passing `json` and the dataset location to a JSON file. All the records are assigned to the `train` split by default, which we would retrieve using the `split` parameter."
      ],
      "metadata": {
        "id": "WxO8yy4StgRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "root = \"/content/drive/MyDrive/Colab Notebooks/torch/\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name_train = root+\"data/BBC-text/bbc-text-train.csv\"\n",
        "dataset_name_test = root+\"data/BBC-text/bbc-text-test.csv\"\n",
        "\n",
        "# Load dataset\n",
        "# il parametro split riguarda il modo in cui sono stati creati i dataset\n",
        "# sono entrambi a train perché abbiamo creato un csv con i dati di train e uno per il test manualmente\n",
        "dataset_train = load_dataset(\"csv\", data_files = dataset_name_train, split = \"train\")\n",
        "dataset_test = load_dataset(\"csv\", data_files = dataset_name_test, split = \"train\")"
      ],
      "metadata": {
        "id": "dDsbh-egsIq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of prompts: {len(dataset_train)}')\n",
        "print(f'Column names are: {dataset_train.column_names}')\n",
        "print('-----')\n",
        "print(f'Number of prompts: {len(dataset_test)}')\n",
        "print(f'Column names are: {dataset_test.column_names}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmiw_YRJsNxS",
        "outputId": "8b026b47-c933-45d8-fc7e-da741ecb6a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of prompts: 1780\n",
            "Column names are: ['instruction', 'input', 'output']\n",
            "-----\n",
            "Number of prompts: 445\n",
            "Column names are: ['instruction', 'input', 'output']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The load_dataset function will convert the CSV file into a dictionary of prompts.\n",
        "# We can look at a random prompt in the dataset using a random index.\n",
        "dataset_train[randrange(len(dataset_train))]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpMGsqP9sSH6",
        "outputId": "32966374-b4a1-409d-e366-dd76e24a57e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Categorize the news article into one of the 5 categories:\\n\\nbusiness\\npolitics\\ntech\\nsport\\nentertainment\\n\\n',\n",
              " 'input': 'o connor aims to grab opportunity johnny o connor is determined to make a big impression when he makes his rbs six nations debut for ireland against scotland on saturday.  the wasps flanker replaces denis leamy but o connor knows that the munster man will be pushing hard for a recall for the following game against england.  it s a  horses for courses  selection really   said o connor.  there s a lot of competition here and i can t just drag my heels around if i don t get picked.  it looks a definite head-to-head battle between himself and 23-year-old leamy - three stone heavier than o connor - for the number seven role against the world champions. nonetheless  all o connor is currently concerned about is making an impression while winning his third cap.   missing the italian game was disappointing certainly  but you can t dwell on these things - it s part and parcel of rugby.  denis has been playing really well and deserved his opportunity.  it s a good situation to be in if there are good players around you  pushing for a place in the side.  o connor  who celebrated his 25th birthday on wednesday  was touted by wasps director of rugby warren gatland as a possible 2005 lions test openside as far back as last september. and his reputation as a breakdown scavenger and heavy hitter has seen him come to the forefront of o sullivan s mind for the scottish tussle. o connor added:  it will be interesting to see how situations on the deck is reffed  with the new laws having come in.   obviously the breakdown a big part of what i do on the pitch so i m hoping to hold some influence there against what is a very solid scottish pack.  o connor will be winning his third cap after making his debut in the victory over south africa last november.',\n",
              " 'output': 'sport'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Prompt Template\n",
        "\n",
        "After loading the instruction dataset, we will define the `create_prompt_formats` function to create a prompt template against each prompt in our dataset and save it in a new dictionary key `text` for further data preprocessing and fine-tuning."
      ],
      "metadata": {
        "id": "5DAcPGV0tWe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\" Creates a formatted prompt template for a prompt in the instruction dataset\n",
        "    :param sample: Prompt or sample from the instruction dataset \"\"\"\n",
        "\n",
        "    # Initialize static strings for the prompt template\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    INSTRUCTION_KEY = \"### Instruction:\"\n",
        "    INPUT_KEY = \"Input:\"\n",
        "    RESPONSE_KEY = \"### Response:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    # Combine a prompt with the static strings\n",
        "    blurb = f\"{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
        "    input_context = f\"{INPUT_KEY}\\n{sample['input']}\" if sample[\"input\"] else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "\n",
        "    # Create a list of prompt template elements\n",
        "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
        "\n",
        "    # Join prompt template elements into a single string to create the prompt template\n",
        "    formatted_prompt = \"\\n\\n\".join(parts)\n",
        "\n",
        "    # Store the formatted prompt template in a new key \"text\"\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "Exb_biggtIdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_prompt_formats(dataset_train[randrange(len(dataset_train))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXR3dYxssYCl",
        "outputId": "618945d2-c552-4aff-ebee-ab769bfff3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'instruction': 'Categorize the news article into one of the 5 categories:\\n\\nbusiness\\npolitics\\ntech\\nsport\\nentertainment\\n\\n',\n",
              " 'input': 'how political squabbles snowball it s become commonplace to argue that blair and brown are like squabbling school kids and that they (and their supporters) need to grow up and stop bickering.  but this analysis in fact gets it wrong. it s not just children who fight - adults do too. and there are solid reasons why even a trivial argument between mature protagonists can be hard to stop once its got going. the key feature of an endless feud is that everyone can agree they d be better off if it ended - but everyone wants to have the last word.  each participant genuinely wants the row to stop  but thinks it worth prolonging the argument just a tiny bit to ensure their view is heard. their successive attempts to end the argument with their last word ensure the argument goes on and on and on. (in the case of mr blair and mr brown  successive books are published  ensuring the issues never die.) now this isn t because the participants are stupid - it s actually each individual behaving entirely rationally  given the incentives facing them. indeed  there s even a piece of economic theory that explains all this. nothing as obscure as  post-neo-classical endogenous growth theory  which the chancellor himself once quoted - but a ubiquitous piece of game theory which all respectable policy wonks are familiar with.  it s often referred to as the  prisoner s dilemma   based on a parable much told in economics degree courses... about a sheriff and two prisoners. the story goes that two prisoners are jointly charged with a heinous crime  and are locked up in separate cells. but the sheriff desperately needs a confession from at least one of them  to provide enough evidence to convict them of the crime. without a confession  the prisoners will get a minimal sentence on some trumped up charge.  clearly the prisoners  best strategy is to keep their mouths shut  and take the short sentence  but the clever sheriff has an idea to induce them to talk. he tells each prisoner separately  that if they confess - and they are the only one to confess - they ll be let off their crime. and he tells them that if they don t confess - and they are the only one not to confess - they ll get life. now  if you are prisoner confronted with this choice  your best bet is to confess. if your partner doesn t confess  you ll get off completely. and if your partner does confess  you d better confess to ensure you don t get life. the result is of course  both prisoners confess  so the sheriff does not have to let either one off. both prisoners  individual logic was to behave that way  even though both would have been better if they had somehow agreed to shut up. don t worry if you don t entirely follow it - you can to look it up on google  where there are 283 000 entries on it.  the prisoners  dilemma and all its ramifications have truly captured economists in the last couple of decades. it is a parable used to describe any situation where there is an obvious sensible choice to be taken collectively  but where the only rational choice individually is to behave selfishly.  a cold war arms race for example - a classic case where both russia and america would be better off with just a few arms  rather than a lot of arms. but as long as each wants just a few more arms than the other  an arms race ensues with the results that the individually logical decision to buy more arms  results in arms levels that are too high. what economics tells us is that once you re in a prisoners  dilemma - unless you are repeating the experience many times over - it s hard to escape the perverse logic of it. it s no good just exhorting people to stop buying arms  or to stop arguing when all their incentives encourage them to carry on. somehow  the incentives have to change.  in the case of the labour party  if you believe the rift between blair and brown camps is as bad as the reports suggest  solomon s wisdom needs to be deployed to solve the problem. every parent knows there are ingenious solutions to arguments  solutions which affect the incentives of the participants. an example  is the famous rule that  one divides  the other chooses  as a way of allocating a piece of cake to be sliced up between greedy children. in the case of an apparently endless argument  if you want it to come to an end  you have to ensure the person who has the last word is one who loses rather than the one who wins the row. the cost of prolonging the row by even one more briefing  or one more book for that matter  has to exceed the benefit of having the last word  and getting your point in. if the rest of the party can enforce that  they ll have the protagonists retreating pretty quickly.',\n",
              " 'output': 'politics',\n",
              " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the news article into one of the 5 categories:\\n\\nbusiness\\npolitics\\ntech\\nsport\\nentertainment\\n\\n\\n\\nInput:\\nhow political squabbles snowball it s become commonplace to argue that blair and brown are like squabbling school kids and that they (and their supporters) need to grow up and stop bickering.  but this analysis in fact gets it wrong. it s not just children who fight - adults do too. and there are solid reasons why even a trivial argument between mature protagonists can be hard to stop once its got going. the key feature of an endless feud is that everyone can agree they d be better off if it ended - but everyone wants to have the last word.  each participant genuinely wants the row to stop  but thinks it worth prolonging the argument just a tiny bit to ensure their view is heard. their successive attempts to end the argument with their last word ensure the argument goes on and on and on. (in the case of mr blair and mr brown  successive books are published  ensuring the issues never die.) now this isn t because the participants are stupid - it s actually each individual behaving entirely rationally  given the incentives facing them. indeed  there s even a piece of economic theory that explains all this. nothing as obscure as  post-neo-classical endogenous growth theory  which the chancellor himself once quoted - but a ubiquitous piece of game theory which all respectable policy wonks are familiar with.  it s often referred to as the  prisoner s dilemma   based on a parable much told in economics degree courses... about a sheriff and two prisoners. the story goes that two prisoners are jointly charged with a heinous crime  and are locked up in separate cells. but the sheriff desperately needs a confession from at least one of them  to provide enough evidence to convict them of the crime. without a confession  the prisoners will get a minimal sentence on some trumped up charge.  clearly the prisoners  best strategy is to keep their mouths shut  and take the short sentence  but the clever sheriff has an idea to induce them to talk. he tells each prisoner separately  that if they confess - and they are the only one to confess - they ll be let off their crime. and he tells them that if they don t confess - and they are the only one not to confess - they ll get life. now  if you are prisoner confronted with this choice  your best bet is to confess. if your partner doesn t confess  you ll get off completely. and if your partner does confess  you d better confess to ensure you don t get life. the result is of course  both prisoners confess  so the sheriff does not have to let either one off. both prisoners  individual logic was to behave that way  even though both would have been better if they had somehow agreed to shut up. don t worry if you don t entirely follow it - you can to look it up on google  where there are 283 000 entries on it.  the prisoners  dilemma and all its ramifications have truly captured economists in the last couple of decades. it is a parable used to describe any situation where there is an obvious sensible choice to be taken collectively  but where the only rational choice individually is to behave selfishly.  a cold war arms race for example - a classic case where both russia and america would be better off with just a few arms  rather than a lot of arms. but as long as each wants just a few more arms than the other  an arms race ensues with the results that the individually logical decision to buy more arms  results in arms levels that are too high. what economics tells us is that once you re in a prisoners  dilemma - unless you are repeating the experience many times over - it s hard to escape the perverse logic of it. it s no good just exhorting people to stop buying arms  or to stop arguing when all their incentives encourage them to carry on. somehow  the incentives have to change.  in the case of the labour party  if you believe the rift between blair and brown camps is as bad as the reports suggest  solomon s wisdom needs to be deployed to solve the problem. every parent knows there are ingenious solutions to arguments  solutions which affect the incentives of the participants. an example  is the famous rule that  one divides  the other chooses  as a way of allocating a piece of cake to be sliced up between greedy children. in the case of an apparently endless argument  if you want it to come to an end  you have to ensure the person who has the last word is one who loses rather than the one who wins the row. the cost of prolonging the row by even one more briefing  or one more book for that matter  has to exceed the benefit of having the last word  and getting your point in. if the rest of the party can enforce that  they ll have the protagonists retreating pretty quickly.\\n\\n### Response:\\npolitics\\n\\n### End'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Maximum Sequence Length of the Pre-trained Model\n",
        "\n",
        "In the next cell, we will define the `get_max_length` function to find out the maximum sequence length of the Llama-2-7B model. This function will pull the model configuration and attempt to find the maximum sequence length from one of the several configuration keys that may contain it. If the maximum sequence length is not found, it will default to 1024. We will use the maximum sequence length during dataset preprocessing to remove records that exceed that context length because the pre-trained model won't accept them."
      ],
      "metadata": {
        "id": "_y2Y5YpVtqGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_length(model):\n",
        "    \"\"\" Extracts maximum token length from the model configuration\n",
        "    :param model: Hugging Face model \"\"\"\n",
        "\n",
        "    # Pull model configuration\n",
        "    conf = model.config\n",
        "    # Initialize a \"max_length\" variable to store maximum sequence length as null\n",
        "    max_length = None\n",
        "    # Find maximum sequence length in the model configuration and save it in \"max_length\" if found\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    # Set \"max_length\" to 1024 (default value) if maximum sequence length is not found in the model configuration\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "    return max_length"
      ],
      "metadata": {
        "id": "A80Ucxettqam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing Dataset Batch\n",
        "\n",
        "The user-defined `preprocess_batch` function will tokenize a batch of the input dataset (`batch`) using the `tokenizer` object. We will set the maximum sequence length using the `max_length` parameter, which will control the maximum length used by the padding or truncation parameter. `truncation = True` will truncate the input to the maximum length provided by the `max_length` parameter. Similarly, `padding = max_length` will pad the input to the maximum length provided. This function will be called in the `preprocess_dataset` function defined next."
      ],
      "metadata": {
        "id": "4mGzskAzt3AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    \"\"\" Tokenizes dataset batch\n",
        "    :param batch: Dataset batch\n",
        "    :param tokenizer: Model tokenizer\n",
        "    :param max_length: Maximum number of tokens to emit from the tokenizer \"\"\"\n",
        "\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length = max_length,\n",
        "        truncation = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "kS6ED02Lt3gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Dataset\n",
        "\n",
        "To preprocess the complete dataset for fine-tuning, we will define the `preprocess_dataset` function, which will perform the following operations:\n",
        "\n",
        "1. Create the formatted prompts against each prompt in the instruction dataset using the `create_prompt_formats` function.\n",
        "2. Tokenize the dataset in batches using the `preprocess_batch` function and removing the original dictionary keys (instruction, input, output, and text).\n",
        "3. Filter out prompts with input token sizes exceeding the maximum length.\n",
        "4. Shuffle the dataset using a random seed."
      ],
      "metadata": {
        "id": "Wjsjh8dwt26Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
        "    \"\"\" Tokenizes dataset for fine-tuning\n",
        "    :param tokenizer (AutoTokenizer): Model tokenizer\n",
        "    :param max_length (int): Maximum number of tokens to emit from the tokenizer\n",
        "    :param seed: Random seed for reproducibility\n",
        "    :param dataset (str): Instruction dataset \"\"\"\n",
        "\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(create_prompt_formats)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched = True,\n",
        "        remove_columns = [\"instruction\", \"input\", \"output\", \"text\"],\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    dataset = dataset.shuffle(seed = seed)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "rd3vBgn6t-xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seed\n",
        "seed = 33\n",
        "\n",
        "max_length = get_max_length(model)\n",
        "preprocessed_dataset_train = preprocess_dataset(tokenizer, max_length, seed, dataset_train)\n",
        "preprocessed_dataset_test = preprocess_dataset(tokenizer, max_length, seed, dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJXCjqiAuOcs",
        "outputId": "d549990d-89d2-4929-8594-f84dca35583b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found max lenth: 4096\n",
            "Preprocessing dataset...\n",
            "Preprocessing dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_dataset_train)\n",
        "print(preprocessed_dataset_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51hVr01-uQT1",
        "outputId": "c517f7b9-e8f2-4b97-93ec-36f200690651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask'],\n",
            "    num_rows: 1778\n",
            "})\n",
            "{'input_ids': [1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 29907, 20440, 675, 278, 9763, 4274, 964, 697, 310, 278, 29871, 29945, 13997, 29901, 13, 13, 8262, 3335, 13, 20087, 1199, 13, 11345, 13, 29879, 637, 13, 296, 13946, 358, 13, 13, 13, 13, 4290, 29901, 13, 29890, 493, 20050, 411, 260, 513, 497, 8494, 15840, 398, 3737, 446, 260, 513, 497, 269, 10823, 756, 1370, 9571, 27683, 896, 505, 2745, 2446, 4723, 304, 11157, 1009, 8078, 5957, 304, 278, 3033, 1049, 767, 470, 12045, 19035, 1075, 304, 263, 17055, 4402, 29889, 29871, 652, 1129, 394, 492, 4083, 540, 756, 4520, 385, 5957, 363, 260, 513, 497, 607, 270, 4495, 5847, 27683, 269, 5376, 322, 393, 1023, 916, 17651, 864, 304, 5193, 29889, 29871, 3737, 446, 947, 451, 864, 304, 748, 964, 278, 4832, 19079, 15982, 292, 1048, 988, 540, 674, 367, 8743, 670, 4402, 20747, 2446, 4259, 259, 394, 492, 5429, 278, 8372, 713, 19656, 29889, 29871, 372, 338, 701, 304, 313, 29890, 493, 12271, 29897, 322, 3973, 3347, 1983, 1742, 29889, 540, 756, 304, 1207, 372, 3799, 29889, 29871, 260, 513, 497, 338, 714, 310, 8078, 472, 278, 1095, 310, 278, 4259, 541, 372, 338, 11098, 393, 3347, 1983, 1742, 338, 18500, 8873, 304, 2867, 278, 4402, 269, 4497, 653, 3829, 304, 24803, 403, 278, 29871, 29906, 29953, 29899, 6360, 29899, 1025, 269, 1261, 4167, 29889, 541, 394, 492, 1663, 2879, 278, 4847, 338, 7088, 901, 1135, 27683, 505, 1925, 373, 278, 1591, 29889, 29871, 3737, 446, 756, 1063, 472, 27683, 363, 9475, 2440, 322, 10753, 304, 3933, 411, 278, 4402, 322, 670, 1261, 4167, 526, 3099, 541, 19163, 573, 259, 278, 10823, 2715, 29889, 29871, 541, 3347, 1983, 1742, 756, 304, 5936, 895, 3737, 446, 269, 995, 322, 591, 864, 304, 8814, 2712, 491, 278, 1095, 310, 2446, 4723, 29889, 13, 13, 2277, 29937, 13291, 29901, 13, 29879, 637, 13, 13, 2277, 29937, 2796], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune(model, tokenizer, preprocessed_dataset_train, preprocessed_dataset_test, lora_config_parameters, training_parameters)\n",
        "#fine_tune(model, tokenizer, dataset_train, lora_config_parameters, training_parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8cwUpFLhxpwl",
        "outputId": "a65cd32f-3dc3-469a-ae0b-7dfe4b836717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA module names: ['v_proj', 'k_proj', 'q_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
            "All Parameters: 3,540,389,888 || Trainable Parameters: 39,976,960 || Trainable Parameters %: 1.1291682911958425\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='57' max='8880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  57/8880 14:03 < 37:35:51, 0.07 it/s, Epoch 0.13/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.024900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.998200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.989500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.094800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.958700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.682800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.754600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.794100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.585500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.744700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.812500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.531800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.630400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.948200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.666200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.544500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.293500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.398200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.566600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.713300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.550700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.622700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.410600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.658900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.470200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.849000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.876400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.477100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.775300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.595200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.776200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.489400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.821100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.537100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.594600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.459200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.467100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.631300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.796100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.664100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.561200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.547800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.668200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.443000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.605900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.459200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.620100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.834900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.402500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.653800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.519000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.646100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.644500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.971200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-28bab710bec9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_dataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_dataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_config_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#fine_tune(model, tokenizer, dataset_train, lora_config_parameters, training_parameters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-df0e71a13459>\u001b[0m in \u001b[0;36mfine_tune\u001b[0;34m(model, tokenizer, dataset_train, dataset_test, lora_config_parameters, training_parameters)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m         )\n\u001b[0;32m-> 1539\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1540\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2663\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2665\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1849\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1851\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1852\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    286\u001b[0m             )\n\u001b[1;32m    287\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;34m\" this checkpoint() is not necessary\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             )\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_with_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_with_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         grads = tuple(\n\u001b[1;32m    290\u001b[0m             \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.43 GiB. GPU 0 has a total capacty of 14.75 GiB of which 1002.81 MiB is free. Process 143389 has 13.52 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLMCMUdqIDN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}