{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu2t3rjWHYIq"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "hf_auth = userdata.get('HF_TOKEN')\n",
        "\n",
        "lm_model_inst = 'meta-llama/Llama-3.2-1B-Instruct'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(lm_model_inst, token=hf_auth)\n",
        "model = AutoModelForCausalLM.from_pretrained(lm_model_inst, token=hf_auth)\n",
        "device = 'cuda'\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "ISgb0FDAHixp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/text_generation#transformers.GenerationConfig\n",
        "\n",
        "def ask(question, model, device):\n",
        "\n",
        "    tk = tokenizer(question, return_tensors='pt')\n",
        "    tk['input_ids'] = tk['input_ids'].to(device)\n",
        "    tk['attention_mask'] = tk['attention_mask'].to(device)\n",
        "\n",
        "    gen_config = GenerationConfig(\n",
        "        do_sample=True,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.0000001)\n",
        "\n",
        "    response = model.generate(\n",
        "        input_ids=tk['input_ids'],\n",
        "        attention_mask=tk['attention_mask'],\n",
        "        generation_config=gen_config)\n",
        "\n",
        "    answer = tokenizer.batch_decode(response[:, len(tk['input_ids'][0]):], skip_special_tokens=True)[0]\n",
        "\n",
        "    return response, answer"
      ],
      "metadata": {
        "id": "5y42i-PBHkVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root = \"/content/gdrive/MyDrive/Colab Notebooks/torch/\"\n",
        "df = pd.read_csv(root+\"data/BBC-text/bbc-text.csv\")\n",
        "\n",
        "# nota, considero le labels testuali, camnio la creazione del dataset\n",
        "labels_list = list(set(df['category']))\n",
        "\n",
        "(x_train, x_test, y_train, y_test) = train_test_split(df['text'], df['category'], test_size=0.2, random_state=17)\n",
        "(x_train, x_val, y_train, y_val) = train_test_split(x_train, y_train, test_size=0.1, random_state=17)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, x, y, stopwords):\n",
        "\n",
        "        # x e y sono series di pandas\n",
        "        tokens_litt = [nltk.word_tokenize(text, language='english')\n",
        "         for text in list(x)]\n",
        "        text_clean = []\n",
        "\n",
        "        if stopwords:\n",
        "            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n",
        "                text_clean.append(' '.join([w.lower() for w in sentence if\n",
        "                    not w.lower() in nltk.corpus.stopwords.words(\"english\")]))\n",
        "        else:\n",
        "            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n",
        "                text_clean.append(' '.join([w.lower() for w in sentence]))\n",
        "            # ogni token è separato dall'altro con uno spazio\n",
        "\n",
        "        self.texts = text_clean\n",
        "        self.labels = [label for label in y]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.texts[idx]\n",
        "        batch_labels = self.labels[idx]\n",
        "\n",
        "        return batch_texts, batch_labels\n",
        "\n",
        "hyperparameters = {\n",
        "    \"epochs\": 5,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": 64,\n",
        "    \"dropout\": 0.1,\n",
        "    \"stopwords\": False,\n",
        "    \"layers\": 1,\n",
        "    \"h_dim\": 300,\n",
        "    \"bilstm\": True,\n",
        "    \"patience\": 5,\n",
        "    \"min_delta\": 0.01\n",
        "}\n",
        "\n",
        "train_dataset = Dataset(x_train, y_train, hyperparameters[\"stopwords\"])\n",
        "val_dataset = Dataset(x_val, y_val, hyperparameters[\"stopwords\"])\n",
        "test_dataset = Dataset(x_test, y_test, hyperparameters[\"stopwords\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80Qp1utmHmDc",
        "outputId": "c72bcd25-e778-4345-d1ec-0f4cd2620142"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Tokenizing ... : 100%|██████████| 1602/1602 [00:00<00:00, 14431.84it/s]\n",
            "Tokenizing ... : 100%|██████████| 178/178 [00:00<00:00, 12662.59it/s]\n",
            "Tokenizing ... : 100%|██████████| 445/445 [00:00<00:00, 15206.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset[0][0])\n",
        "print(test_dataset[0][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWMbE0I_H4yS",
        "outputId": "7f198214-d186-4853-fb61-b5497948d689"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "butler strikes gold in spain britain s kathy butler continued her impressive year with victory in sunday s 25th cross internacional de venta de banos in spain . the scot who led gb to world cross country bronze earlier this year moved away from the field with ines monteiro halfway into the 6.6km race . she then shrugged off her portuguese rival to win in 20 minutes 38 seconds . meanwhile briton karl keska battled bravely to finish seventh in the men s 10.6km race in a time of 31:41. kenenisa bekele of ethiopia - the reigning world long and short course champion - was never troubled by any of the opposition winning leisurely in 30.26. butler said of her success : i felt great throughout the race and hope this is a good beginning for a marvellous 2005 season for me . elsewhere abebe dinkessa of ethiopia won the brussels iaaf cross-country race on sunday completing the 10 500m course in 33.22. gelete burka then crowned a great day for ethiopia by claiming victory in the women s race .\n",
            "sport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {'role': 'system', 'content': 'Classify the following article in one of the folllowing categories: business, politics, tech, sport or entertainment'},\n",
        "    {'role': 'user', 'content': test_dataset[0][0]}\n",
        "]\n",
        "\n",
        "print(tokenizer.apply_chat_template(messages, tokenize=False))"
      ],
      "metadata": {
        "id": "SwxoCQiXJe5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {'role': 'system', 'content': 'Classify the following article in one of the folllowing categories: business, politics, tech, sport or entertainment'},\n",
        "    {'role': 'user', 'content': test_dataset[0][0]}\n",
        "]\n",
        "\n",
        "response, answer = ask(tokenizer.apply_chat_template(messages, tokenize=False), model, device)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "qiZ7HF7sJlCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate over the entiere dataset\n",
        "\n",
        "results = []\n",
        "\n",
        "for idx, (texts, labels) in enumerate(tqdm(test_dataset, desc='test set')):\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'Classify the following article in one of the folllowing categories: business, politics, tech, sport or entertainment'},\n",
        "        {'role': 'user', 'content': texts}\n",
        "    ]\n",
        "\n",
        "    response, answer = ask(tokenizer.apply_chat_template(messages, tokenize=False), model, device)\n",
        "    results.append((answer, labels))"
      ],
      "metadata": {
        "id": "4Xm_1u3mJn-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results[0])\n",
        "print(results[1])\n",
        "print(results[2])\n",
        "print(results[3])"
      ],
      "metadata": {
        "id": "GB9cAqgCJuh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sistemo l'output, normalizzo le predizione rispetto alla presenza di una label\n",
        "\n",
        "goldens = []\n",
        "pred_fixed = []\n",
        "\n",
        "for pred, golden in results:\n",
        "    goldens.append(golden)\n",
        "    preds_lower = pred.lower()\n",
        "\n",
        "    found = False\n",
        "    for l in labels_list:\n",
        "        if l in preds_lower:\n",
        "            pred_fixed.append(l)\n",
        "            found = True\n",
        "            break\n",
        "    if not found:\n",
        "        pred_fixed.append('')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(goldens, pred_fixed))"
      ],
      "metadata": {
        "id": "FJ3XwXs8J0hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constrain decoding, forzo il modello a restituire delle parole specifiche, le nostre label\n",
        "\n",
        "labels_tok = [[tokenizer(' '+l, add_special_tokens=False)['input_ids']] for l in labels_list]\n",
        "print(labels_tok)"
      ],
      "metadata": {
        "id": "2GD4PW8zJ3dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_constrain(question, model, device, force_words_ids):\n",
        "    tk = tokenizer(question, return_tensors='pt')\n",
        "    tk['input_ids'] = tk['input_ids'].to(device)\n",
        "    tk['attention_mask'] = tk['attention_mask'].to(device)\n",
        "\n",
        "    gen_config = GenerationConfig(\n",
        "        force_words_ids=force_words_ids,\n",
        "        max_new_tokens=10,\n",
        "        temperature = 0.000001\n",
        "        num_beams=2,\n",
        "    )\n",
        "\n",
        "    response = model.generate(\n",
        "        input_ids=tk['input_ids'],\n",
        "        attention_mask=tk['attention_mask'],\n",
        "        generation_config=gen_config)\n",
        "\n",
        "    answer = tokenizer.batch_decode(response[:, len(tk['input_ids'][0]):], skip_special_tokens=True)[0]\n",
        "\n",
        "    return response, answer"
      ],
      "metadata": {
        "id": "3Yx1xgTuLeNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ridefinisco anche l'input, scrivo parte della risposta del LLM seguendo il chat template\n",
        "# più info sul formato di prompt per i singoli modelli sulla scheda del modello o sul paper pubblicato di riferimento\n",
        "# vedere qua per Llama 3\n",
        "    # https://www.llama.com/docs/how-to-guides/prompting#prompting\n",
        "    # https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1#prompt-template\n",
        "\n",
        "tokenizer.apply_chat_template(messages, tokenize=False)+'<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI would classify the article as:'"
      ],
      "metadata": {
        "id": "U7JXCbJGLcia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_2 = []\n",
        "\n",
        "for idx, (texts, labels) in enumerate(tqdm(test_dataset, desc='test set')):\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': 'Classify the following article in one of the folllowing categories: business, politics, tech, sport or entertainment'},\n",
        "        {'role': 'user', 'content': texts}\n",
        "    ]\n",
        "\n",
        "    response, answer = ask_constrain(tokenizer.apply_chat_template(messages, tokenize=False)+'<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI would classify the article as:', model, device, labels_tok)\n",
        "    results_2.append((answer, labels))\n"
      ],
      "metadata": {
        "id": "h4qinBaJLjnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_2[0])\n",
        "print(results_2[1])\n",
        "print(results_2[2])\n",
        "print(results_2[3])"
      ],
      "metadata": {
        "id": "D4Yly7zjLryZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goldens_2 = []\n",
        "pred_fixed_2 = []\n",
        "\n",
        "for pred, golden in results:\n",
        "    goldens.append(golden)\n",
        "    preds_lower = pred.lower()\n",
        "\n",
        "    found = False\n",
        "    for l in list(labels_list):\n",
        "        if l in preds_lower:\n",
        "            pred_fixed.append(l)\n",
        "            found = True\n",
        "            break\n",
        "    if not found:\n",
        "        pred_fixed.append('')\n",
        "\n",
        "print(classification_report(goldens_2, pred_fixed_2))"
      ],
      "metadata": {
        "id": "RktWm67bLnw6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}