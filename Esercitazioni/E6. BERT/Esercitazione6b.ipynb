{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "o_9x1m3YC3RF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5jGMRPJ1E1e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Dataset\n",
        "# repository https://github.com/iresiragusa/NLP/tree/main\n",
        "# https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category?select=bbc-text.csv\n",
        "# scarichiamo il dataset e lo carichiamo su COLAB\n",
        "\n",
        "root = \"/content/gdrive/MyDrive/Colab Notebooks/torch/\"\n",
        "df = pd.read_csv(root+\"data/BBC-text/bbc-text.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# associo ad ogni categoria un indice, così ho delle label numeriche\n",
        "labels_dict = {\n",
        "    'business': 0,\n",
        "    'politics': 1,\n",
        "    'tech': 2,\n",
        "    'sport': 3,\n",
        "    'entertainment': 4\n",
        "}\n",
        "\n",
        "df['labels'] = df.apply(lambda row: labels_dict[row.category], axis = 1)"
      ],
      "metadata": {
        "id": "ystNFP_lfgwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, x_test, y_train, y_test) = train_test_split(df['text'], df['labels'], test_size=0.2, random_state=17)\n",
        "\n",
        "(x_train, x_val, y_train, y_val) = train_test_split( x_train, y_train, test_size=0.1, random_state=17)"
      ],
      "metadata": {
        "id": "s16PrHdD1bs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, x, y, stopwords):\n",
        "\n",
        "        if stopwords:\n",
        "            tokens_litt = [nltk.word_tokenize(text, language='english') for text in list(x)]\n",
        "            text_clean = []\n",
        "            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n",
        "                text_clean.append(' '.join([w for w in sentence if not w.lower() in nltk.corpus.stopwords.words(\"english\")]))\n",
        "        else:\n",
        "            tokens_litt = [nltk.word_tokenize(text, language='english') for text in list(x)]\n",
        "            text_clean = []\n",
        "            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n",
        "                #sentence_clean = ' '.join([w.lower() for w in sentence])\n",
        "                #text_clean.append(sentence_clean)\n",
        "                text_clean.append(' '.join([w.lower() for w in sentence]))\n",
        "            # ogni token è separato dall'altro con uno spazio\n",
        "        self.texts = [text for text in text_clean]\n",
        "        self.labels = [torch.tensor(label) for label in y]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_labels = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_labels"
      ],
      "metadata": {
        "id": "WlYWKSjDeQi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    \"epochs\": 5,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"batch_size\": 32,\n",
        "    \"dropout\": 0.1,\n",
        "    #\"stopwords\": True,\n",
        "    \"stopwords\": False,\n",
        "    \"h_dim\": 768,\n",
        "    \"patience\": 5,\n",
        "    \"min_delta\": 0.01,\n",
        "    \"language_model\": \"bert-base-uncased\"\n",
        "}"
      ],
      "metadata": {
        "id": "TdXojbqsejFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creo i dataset\n",
        "\n",
        "train_dataset = Dataset(x_train, y_train, hyperparameters[\"stopwords\"])\n",
        "val_dataset = Dataset(x_val, y_val, hyperparameters[\"stopwords\"])\n",
        "test_dataset = Dataset(x_test, y_test, hyperparameters[\"stopwords\"])"
      ],
      "metadata": {
        "id": "WbuEU9Xzevlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassifierDeep(nn.Module):\n",
        "\n",
        "    def __init__(self, labels, hdim, dropout, model_name):\n",
        "        super(ClassifierDeep, self).__init__()\n",
        "        config = AutoConfig.from_pretrained(model_name)\n",
        "        self.lm_model = AutoModel.from_pretrained(model_name, config=config)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hdim, hdim),\n",
        "            nn.BatchNorm1d(hdim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hdim, labels),\n",
        "            )\n",
        "\n",
        "    def forward(self, input_id_text, attention_mask):\n",
        "        output = self.lm_model(input_id_text, attention_mask).last_hidden_state\n",
        "        return self.classifier(output[:,0,:])"
      ],
      "metadata": {
        "id": "qrQ38Hr5a_xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0.0):\n",
        "\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta              # valore minimo di decrescita della loss di validazione all'epoca corrente\n",
        "                                                # per asserire che c'è un miglioramenti della loss\n",
        "        self.counter = 0                        # contatore delle epoche di pazienza\n",
        "        self.early_stop = False                 # flag di early stop\n",
        "        self.min_validation_loss = torch.inf    # valore corrente ottimo della loss di validazione\n",
        "\n",
        "    def __call__(self, validation_loss):\n",
        "        # chiamata in forma funzionale dell'oggetto di classe EarlySopping\n",
        "\n",
        "        if (validation_loss + self.min_delta) >= self.min_validation_loss:  # la loss di validazione non decresce\n",
        "            self.counter += 1                                               # incrementiamo il contatore delle epoche di pazienza\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(\"Early stop!\")\n",
        "        else:                                                               # c'è un miglioramento della loss:\n",
        "            self.min_validation_loss = validation_loss                      # consideriamo la loss corrente\n",
        "                                                                            # come nuova loss ottimale\n",
        "            self.counter = 0                                                # e azzeriamo il contatore di pazienza"
      ],
      "metadata": {
        "id": "YeLXqnCIgc9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, dataloader, tokenizer, loss, optimizer, device):\n",
        "    model.train()\n",
        "\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch_texts, batch_labels in tqdm(dataloader, desc='training set'):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        tokens = tokenizer(list(batch_texts), add_special_tokens=True,\n",
        "                            return_tensors='pt', padding='max_length',\n",
        "                            max_length = 512, truncation=True)\n",
        "        input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
        "        mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        output = model(input_id_texts, mask_texts)\n",
        "\n",
        "        # la loss è una CrossEntropyLoss, al suo interno ha la logsoftmax + negative log likelihood loss\n",
        "        batch_loss = loss(output, batch_labels)\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += batch_loss.item()\n",
        "\n",
        "        # per calcolare l'accuracy devo generare le predizioni applicando manualmente la logsoftmax\n",
        "        softmax = nn.LogSoftmax(dim=1)\n",
        "        epoch_acc += (softmax(output).argmax(dim=1) == batch_labels).sum().item()\n",
        "\n",
        "        batch_labels = batch_labels.detach().cpu()\n",
        "        input_id_texts = input_id_texts.detach().cpu()\n",
        "        mask_texts = mask_texts.detach().cpu()\n",
        "        output = output.detach().cpu()\n",
        "\n",
        "    return epoch_loss/len(dataloader), epoch_acc"
      ],
      "metadata": {
        "id": "eRwTy5yVgoCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(model, dataloader, tokenizer, loss, device):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_texts, batch_labels, in tqdm(dataloader, desc='dev set'):\n",
        "\n",
        "            tokens = tokenizer(list(batch_texts), add_special_tokens=True,\n",
        "                               return_tensors='pt', padding='max_length',\n",
        "                               max_length = 512, truncation=True)\n",
        "            input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n",
        "            mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            output = model(input_id_texts, mask_texts)\n",
        "\n",
        "            batch_loss = loss(output, batch_labels)\n",
        "            epoch_loss += batch_loss.item()\n",
        "\n",
        "            softmax = nn.LogSoftmax(dim=1)\n",
        "            epoch_acc += (softmax(output).argmax(dim=1) == batch_labels).sum().item()\n",
        "\n",
        "            batch_labels = batch_labels.detach().cpu()\n",
        "            input_id_texts = input_id_texts.detach().cpu()\n",
        "            mask_texts = mask_texts.detach().cpu()\n",
        "            output = output.detach().cpu()\n",
        "\n",
        "    return epoch_loss/len(dataloader), epoch_acc"
      ],
      "metadata": {
        "id": "Jo7jZ2PDuRAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test(model, epochs, optimizer, device, train_data, test_data,\n",
        "               batch_size, language_model, train_loss_fn, test_loss_fn=None,\n",
        "               early_stopping=None, val_data=None, scheduler=None):\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "    # check sulle funzioni di loss\n",
        "    if test_loss_fn == None:\n",
        "        test_loss_fn = train_loss_fn\n",
        "\n",
        "    # liste dei valori di loss e accuracy epoca per epoca per il plot\n",
        "    train_loss = []\n",
        "    validation_loss = []\n",
        "    test_loss = []\n",
        "\n",
        "    train_acc = []\n",
        "    validation_acc = []\n",
        "    test_acc = []\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(language_model)\n",
        "\n",
        "    # Ciclo di addestramento con early stopping\n",
        "    for epoch in tqdm(range(1,epochs+1)):\n",
        "\n",
        "        epoch_train_loss, epoch_train_acc = train_loop(model,\n",
        "                    train_dataloader, tokenizer, train_loss_fn, optimizer, device)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "        train_acc.append(epoch_train_acc/len(train_data))\n",
        "\n",
        "        # validation se è presente la callback di early stopping\n",
        "        if early_stopping != None:\n",
        "                epoch_validate_loss, epoch_validate_acc = test_loop(model,\n",
        "                                val_dataloader, tokenizer, test_loss_fn, device)\n",
        "                validation_loss.append(epoch_validate_loss)\n",
        "                validation_acc.append(epoch_validate_acc/len(val_data))\n",
        "\n",
        "        # test\n",
        "        epoch_test_loss, epoch_test_acc,= test_loop(model,\n",
        "                                test_dataloader, tokenizer, test_loss_fn, device)\n",
        "        test_loss.append(epoch_test_loss)\n",
        "        test_acc.append(epoch_test_acc/len(test_data))\n",
        "\n",
        "        val_loss_str = f'Validation loss: {epoch_validate_loss:6.4f} ' if early_stopping != None else ' '\n",
        "                        # ' if early_stopping != None else ' '\n",
        "        val_acc_str = f'Validation accuracy: {(epoch_validate_acc/len(val_data)):6.4f} ' if early_stopping != None else ' '\n",
        "                        # ' if early_stopping != None else ' '\n",
        "        print(f\"\\nTrain loss: {epoch_train_loss:6.4f} {val_loss_str}Test loss: {epoch_test_loss:6.4f}\")\n",
        "                        # Test loss: {epoch_test_loss:6.4f}\")\n",
        "        print(f\"Train accuracy: {(epoch_train_acc/len(train_data)):6.4f} {val_acc_str}Test accuracy: {(epoch_test_acc/len(test_data)):6.4f}\")\n",
        "                        # {val_acc_str}Test accuracy:\n",
        "                        # {(epoch_test_acc/len(test_data)):6.4f}\")\n",
        "\n",
        "        # early stopping\n",
        "        if early_stopping != None:\n",
        "                early_stopping(epoch_validate_loss)\n",
        "                if early_stopping.early_stop:\n",
        "                    break\n",
        "\n",
        "    return train_loss, validation_loss, test_loss, train_acc, validation_acc, test_acc\n",
        "                        # train_acc, validation_acc, test_acc"
      ],
      "metadata": {
        "id": "W_urLi5MubL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Acquisiamo il device su cui effettueremo il training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "model = ClassifierDeep(len(labels_dict),\n",
        "                    hyperparameters[\"h_dim\"],\n",
        "                    hyperparameters[\"dropout\"],\n",
        "                    hyperparameters[\"language_model\"]).to(device)\n",
        "print(model)\n",
        "\n",
        "# Calcoliamo il numero totale dei parametri del modello\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Numbero totale dei parametri: {total_params}\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "\n",
        "# Creiamo la callback di early stopping da passare al nostro metodo di addestramento\n",
        "early_stopping = EarlyStopping(patience=hyperparameters['patience'],\n",
        "                               min_delta=hyperparameters['min_delta'])\n"
      ],
      "metadata": {
        "id": "ozVTHJGFvjEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Routine di addestramento\n",
        "train_loss, validation_loss,test_loss, train_acc, validation_acc, test_acc = train_test(model,\n",
        "                                                # train_test(model,\n",
        "                                                hyperparameters['epochs'],\n",
        "                                                #50,\n",
        "                                                optimizer,\n",
        "                                                device,\n",
        "                                                train_dataset,\n",
        "                                                test_dataset,\n",
        "                                                hyperparameters['batch_size'],\n",
        "                                                hyperparameters['language_model'],\n",
        "                                                criterion,\n",
        "                                                criterion,\n",
        "                                                early_stopping,\n",
        "                                                val_dataset)"
      ],
      "metadata": {
        "id": "aHT4iHdawXrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss, label='training loss')\n",
        "plt.plot(validation_loss, label='validation loss')\n",
        "plt.plot(test_loss, label='test loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0,4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Iec8xbPLJheR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_acc, label='training accuracy')\n",
        "plt.plot(validation_acc, label='validation accuracy')\n",
        "plt.plot(test_acc, label='test accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0,2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_w9lnCY3JiPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}